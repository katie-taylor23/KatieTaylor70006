{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "sys.setrecursionlimit(1500)\n",
    "\n",
    "# Importing the packages\n",
    "import argparse\n",
    "import time\n",
    "import torch\n",
    "import dgl\n",
    "import dgl.nn as dglnn\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl import AddSelfLoop\n",
    "from dgl.data import CiteseerGraphDataset, CoraGraphDataset, PubmedGraphDataset\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting graph first as networkx object and then as .graphml file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from neo4j import GraphDatabase\n",
    "import dgl\n",
    "import numpy as np\n",
    "\n",
    "# Neo4j database connection details\n",
    "uri = \"bolt://localhost:7687\"\n",
    "username = \"neo4j\"\n",
    "password = \"ndTmsTkmT333!\"\n",
    "\n",
    "driver = GraphDatabase.driver(uri, auth=(username, password))\n",
    "\n",
    "# Query to retrieve nodes and relationships\n",
    "query = '''\n",
    "MATCH (n:Gene)-[r:CHROM_INT]->(m:Gene)\n",
    "RETURN n, r, m, n.difcase, m.difcase, n.ATAC, m.ATAC, n.ATACTR, m.ATACTR, n.Dif_ATAC, m.Dif_ATAC, n.H3K27Ac, m.H3K27Ac, n.H3K27AcTR, m.H3K27AcTR, n.Dif_H3K27Ac, m.Dif_H3K27Ac, n.H3K4Me1, m.H3K4Me1, n.H3K4Me1TR, m.H3K4Me1TR, n.Dif_H3K4Me1, m.Dif_H3K4Me1, n.H3K4Me3, m.H3K4Me3, n.H3K4Me3TR, m.H3K4Me3TR, n.Dif_H3K4Me3, m.Dif_H3K4Me3, n.ISGverse, m.ISGverse, n.HGNCnum, m.HGNCnum;\n",
    "'''\n",
    "\n",
    "def process_attribute(value):\n",
    "    if value in [None, 'NA']:\n",
    "        return 0\n",
    "    try:\n",
    "        return float(value)\n",
    "    except ValueError:\n",
    "        return 0\n",
    "\n",
    "with driver.session() as session:\n",
    "    result = session.run(query)\n",
    "    data = result.data()\n",
    "\n",
    "nx_graph = nx.Graph()\n",
    "\n",
    "for row in data:\n",
    "    # Extract the nodes and relationships from the query result\n",
    "    node1 = row['n']\n",
    "    rel = row['r']\n",
    "    node2 = row['m']\n",
    "    \n",
    "    # Access the properties with a default value of 0 if not present\n",
    "    node1_id = node1.get('Ensemblid', 'unknown1')\n",
    "    node2_id = node2.get('Ensemblid', 'unknown2')\n",
    "\n",
    "    # Extract attributes, replacing 'NA' with 0\n",
    "    attributes_n = [\n",
    "        process_attribute(node1.get('ATAC', 0)), process_attribute(node1.get('ATACTR', 0)), \n",
    "        process_attribute(node1.get('Dif_ATAC', 0)), process_attribute(node1.get('H3K27Ac', 0)),\n",
    "        process_attribute(node1.get('H3K27AcTR', 0)), process_attribute(node1.get('Dif_H3K27Ac', 0)),\n",
    "        process_attribute(node1.get('H3K4Me1', 0)), process_attribute(node1.get('H3K4Me1TR', 0)), \n",
    "        process_attribute(node1.get('Dif_H3K4Me1', 0)), process_attribute(node1.get('H3K4Me3', 0)),\n",
    "        process_attribute(node1.get('H3K4Me3TR', 0)), process_attribute(node1.get('Dif_H3K4Me3', 0))\n",
    "    ]\n",
    "\n",
    "    attributes_m = [\n",
    "        process_attribute(node2.get('ATAC', 0)), process_attribute(node2.get('ATACTR', 0)), \n",
    "        process_attribute(node2.get('Dif_ATAC', 0)), process_attribute(node2.get('H3K27Ac', 0)), \n",
    "        process_attribute(node2.get('H3K27AcTR', 0)), process_attribute(node2.get('Dif_H3K27Ac', 0)), \n",
    "        process_attribute(node2.get('H3K4Me1', 0)), process_attribute(node2.get('H3K4Me1TR', 0)), \n",
    "        process_attribute(node2.get('Dif_H3K4Me1', 0)), process_attribute(node2.get('H3K4Me3', 0)),\n",
    "        process_attribute(node2.get('H3K4Me3TR', 0)), process_attribute(node2.get('Dif_H3K4Me3', 0))\n",
    "    ]\n",
    "\n",
    "    # Add nodes to the graph\n",
    "    nx_graph.add_node(node1_id, difcase=process_attribute(row.get('n.difcase', 0)), attributes=attributes_n)\n",
    "    nx_graph.add_node(node2_id, difcase=process_attribute(row.get('m.difcase', 0)), attributes=attributes_m)\n",
    "    \n",
    "    # Add edges to the graph\n",
    "    nx_graph.add_edge(node1_id, node2_id, relationship=type(rel).__name__)\n",
    "\n",
    "my_dgl_graph = dgl.from_networkx(nx_graph, node_attrs=['difcase', 'attributes'])\n",
    "\n",
    "dgl.save_graphs('~/70006/graph_MCF7_epiaug6.dgl', [my_dgl_graph])\n",
    "print(my_dgl_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchdata\n",
    "print(\"torch version:\", torch.__version__)\n",
    "print(\"torchdata version:\", torchdata.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#graphs, _=dgl.load_graphs(\"/rds/general/user/kmt23/home/jobs/epiaug4new.dgl\")\n",
    "graphs, _=dgl.load_graphs(\"/rds/general/user/kmt23/home/jobs/epiaug6new.dgl\")\n",
    "my_dgl_graph = graphs[0]\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting the features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = my_dgl_graph.ndata['difcase']\n",
    "features = my_dgl_graph.ndata['attributes']\n",
    "num_nodes = my_dgl_graph.num_nodes()\n",
    "num_edges = my_dgl_graph.num_edges()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting the training and testing masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create random masks\n",
    "# create train, validation and test masks - these define which nodes will be used for training, validation or testing\n",
    "train_mask = torch.zeros(num_nodes, dtype=bool)\n",
    "val_mask = torch.zeros(num_nodes, dtype=bool)\n",
    "test_mask = torch.zeros(num_nodes, dtype=bool)\n",
    "# First split: train+val and test\n",
    "train_val_indices, test_indices = train_test_split(torch.arange(num_nodes), test_size=0.2, random_state=42)\n",
    "# Second split: train and val\n",
    "train_indices, val_indices = train_test_split(train_val_indices, test_size=0.25, random_state=42)  # 0.25 * 0.8 = 0.2\n",
    "# Set the mask values to True for the corresponding indices\n",
    "train_mask[train_indices] = True\n",
    "val_mask[val_indices] = True\n",
    "test_mask[test_indices] = True\n",
    "#masks = torch.from_numpy(train_mask), torch.from_numpy(val_mask), torch.from_numpy(test_mask)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#train_mask = torch.zeros(num_nodes, dtype=torch.bool).bernoulli(0.6)\n",
    "#val_mask = torch.zeros(num_nodes, dtype=torch.bool).bernoulli(0.2) # make these not overlap\n",
    "#test_mask = ~(train_mask | val_mask)\n",
    "    \n",
    "    # Add masks to the graph\n",
    "my_dgl_graph.ndata['train_mask'] = train_mask\n",
    "my_dgl_graph.ndata['val_mask'] = val_mask\n",
    "my_dgl_graph.ndata['test_mask'] = test_mask\n",
    "masks = (train_mask, val_mask, test_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and testing the GAT - this was done on the HPC with one GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import time\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import dgl\n",
    "import dgl.nn as dglnn\n",
    "from dgl import AddSelfLoop\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "class GAT(nn.Module):\n",
    "    def __init__(self, in_size, hid_size, out_size, heads):\n",
    "        super(GAT, self).__init__()\n",
    "        self.gat_layers = nn.ModuleList()\n",
    "        # Two-layer GAT\n",
    "        self.gat_layers.append(\n",
    "            dglnn.GATConv(\n",
    "                in_size, hid_size, heads[0], feat_drop=0.05, attn_drop=0.05, activation=F.elu\n",
    "            )\n",
    "        )\n",
    "        self.gat_layers.append(\n",
    "            dglnn.GATConv(\n",
    "                hid_size * heads[0], out_size, heads[1], feat_drop=0.05, attn_drop=0.05, activation=None\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def forward(self, g, inputs):\n",
    "        h = inputs\n",
    "        for i, layer in enumerate(self.gat_layers):\n",
    "            h = layer(g, h)\n",
    "            if i == len(self.gat_layers) - 1:  # last layer\n",
    "                h = h.mean(1)\n",
    "            else:  # other layer(s)\n",
    "                h = h.flatten(1)\n",
    "        return h\n",
    "\n",
    "\n",
    "def evaluate(g, features, labels, mask, model):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(g, features)\n",
    "        logits = logits[mask]\n",
    "        labels = labels[mask]\n",
    "        _, indices = torch.max(logits, dim=1)\n",
    "        correct = torch.sum(indices == labels)\n",
    "        return correct.item() * 1.0 / len(labels)\n",
    "\n",
    "\n",
    "def train(graph, features, labels, masks, model, num_epochs, patience, class_weights):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    loss_fcn = nn.CrossEntropyLoss(weight=class_weights)  # CrossEntropyLoss with class weights\n",
    "    \n",
    "    train_losses = []  # List to store training losses\n",
    "    val_losses = []  # List to store validation losses\n",
    "    best_val_loss = float('inf')\n",
    "    best_epoch = -1\n",
    "    best_model_state = None\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        t0 = time.time()\n",
    "        model.train()\n",
    "        logits = model(graph, features)\n",
    "        # logits = model.forward(graph, features)\n",
    "        loss = loss_fcn(logits[masks[0]], labels[masks[0]].long())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Validation step\n",
    "        val_loss = evaluate_loss(graph, features, labels, masks[1], model, loss_fcn)\n",
    "\n",
    "        # Collect the loss for this epoch\n",
    "        train_losses.append(loss.item())\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_epoch = epoch\n",
    "            best_model_state = deepcopy(model.state_dict())\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        t1 = time.time()\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}, Val Loss: {val_loss:.4f}, Time: {t1 - t0:.4f}')\n",
    "\n",
    "        # Early stopping check\n",
    "        if patience_counter >= patience:\n",
    "            print(f'Early stopping at epoch {epoch+1}')\n",
    "            break\n",
    "\n",
    "    # Load the best model state\n",
    "    model.load_state_dict(best_model_state)\n",
    "\n",
    "    return train_losses, val_losses, best_epoch, best_val_loss\n",
    "\n",
    "\n",
    "def evaluate_loss(g, features, labels, mask, model, loss_fcn):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(g, features)\n",
    "        logits = logits[mask]\n",
    "        labels = labels[mask]\n",
    "        loss = loss_fcn(logits, labels.long())\n",
    "        return loss.item()\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    if \"ipykernel_launcher\" in sys.argv[0]:\n",
    "        sys.argv = [sys.argv[0]]\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--num_epochs\", type=int, default=4000, help=\"Number of epochs for training.\")\n",
    "    parser.add_argument(\"--num_gpus\", type=int, default=1, help=\"Number of GPUs used for training and evaluation.\")\n",
    "    parser.add_argument(\"--patience\", type=int, default=100, help=\"Early stopping patience.\")\n",
    "    return parser.parse_args()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    args = parse_args()\n",
    "    \n",
    "    print(\"Training with custom DGL graph.\")\n",
    "    \n",
    "    # Masks need to be predefined or generated appropriately\n",
    "    # my_dgl_graph, features, labels, and masks should be defined appropriately here\n",
    "\n",
    "    # Check if GPU is available\n",
    "    if args.num_gpus > 0 and torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "    \n",
    "    my_dgl_graph = my_dgl_graph.to(device)\n",
    "    features = features.to(device)\n",
    "    labels = labels.to(device)\n",
    "\n",
    "    # Ensure masks are on the same device\n",
    "    masks = [mask.to(device) for mask in masks]\n",
    "\n",
    "    # Create GAT model\n",
    "    in_size = features.shape[1]\n",
    "    out_size = 3  # Predicting a categorical label with 3 classes\n",
    "    model = GAT(in_size, 100, out_size, heads=[8, 8]).to(device)\n",
    "\n",
    "    # Define class weights, with higher weight for the \"difcase\" category (assuming it's class 1)\n",
    "    class_weights = torch.tensor([1.0, 1.0, 1.5], device=device)\n",
    "\n",
    "    print(\"Training...\")\n",
    "    train_losses, val_losses, best_epoch, best_val_loss = train(my_dgl_graph, features, labels, masks, model, args.num_epochs, args.patience, class_weights)\n",
    "\n",
    "    print(f'Best Epoch: {best_epoch+1}, Best Validation Loss: {best_val_loss:.4f}')\n",
    "\n",
    "    print(\"Testing...\")\n",
    "    acc = evaluate(my_dgl_graph, features, labels, masks[2], model)\n",
    "    print(\"Test accuracy {:.4f}\".format(acc))\n",
    "\n",
    "    # Plot and save training and validation losses\n",
    "    plt.plot(range(len(train_losses)), train_losses, label='Train Loss')\n",
    "    plt.plot(range(len(val_losses)), val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Losses')\n",
    "    plt.legend()\n",
    "    plt.savefig('train_val_lossesAug4.svg')\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(range(len(train_losses[-300:])), train_losses[-300:], label='Train Loss')\n",
    "    plt.plot(range(len(val_losses[-300:])), val_losses[-300:], label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Losses (Last 300 Epochs)')\n",
    "    plt.legend()\n",
    "    plt.savefig('train_val_losses_last_300Aug4.svg')\n",
    "    print('Train losses:', np.round(train_losses, 3))\n",
    "    print('Validation losses:', np.round(val_losses, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(g, features, labels, mask, model):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(g, features)\n",
    "        logits = logits[mask]\n",
    "        labels = labels[mask]\n",
    "        _, predicted = torch.max(logits, dim=1)\n",
    "        correct = torch.sum(predicted == labels)\n",
    "        accuracy = correct.item() * 1.0 / len(labels)\n",
    "    return accuracy, logits, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(state_dict, \"best_mcf7gat_modelaug4n.pth\")\n",
    "\n",
    "print(\"Model state dictionary saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(g, features, mask, model):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(g, features)\n",
    "        logits = logits[mask]\n",
    "        _, predicted = torch.max(logits, dim=1)\n",
    "        return predicted.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions(predictions, true_values, title):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(range(len(true_values)), true_values, alpha=0.5, label='True Values')\n",
    "    plt.scatter(range(len(predictions)), predictions, alpha=0.5, label='Predictions')\n",
    "    plt.xlabel('Sample Index')\n",
    "    plt.ylabel('Class Label')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testing...\")\n",
    "test_acc, test_logits, test_labels = evaluate(my_dgl_graph, features, labels, masks[2], model)\n",
    "print(\"Test accuracy {:.4f}\".format(test_acc))\n",
    "\n",
    " # Get predictions for test set\n",
    "predictions = get_predictions(my_dgl_graph, features, masks[2], model)\n",
    "true_values = test_labels.cpu().numpy()\n",
    "\n",
    "    # Plot predictions vs true values\n",
    "plot_predictions(predictions[:], true_values[:], 'Predictions vs. True Values (Test Set)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def component_eval(true_values, predictions):\n",
    "    print(\"Total number of samples = \")\n",
    "    print(len(true_values))\n",
    "    print(\"Number of true 0: \")\n",
    "    true_0 = np.where(true_values == 0)[0]\n",
    "    print(len(true_0))\n",
    "    print(\"Correctly identified as 0: \")\n",
    "    preds_0 = np.where(predictions == 0)[0]\n",
    "    \n",
    "    counter = 0\n",
    "    for i in range(len(preds_0)):\n",
    "        if preds_0[i] in true_0:\n",
    "            counter += 1\n",
    "    print(counter)\n",
    "    print(\"True 0, identified as 1: \")\n",
    "    preds_1 = np.where(predictions == 1)[0]\n",
    "    counter = 0\n",
    "    for i in range(len(preds_1)):\n",
    "        if preds_1[i] in true_0:\n",
    "            counter += 1\n",
    "    print(counter)\n",
    "    print(\"Number of true 1: \")\n",
    "    true_1 = np.where(true_values == 1)[0]\n",
    "    print(len(true_1))\n",
    "    print(\"Correctly identified as 1: \")\n",
    "    preds_1 = np.where(predictions == 1)[0]\n",
    "    \n",
    "    counter = 0\n",
    "    for i in range(len(preds_1)):\n",
    "        if preds_1[i] in true_1:\n",
    "            counter += 1\n",
    "    print(counter)\n",
    "    \n",
    "    print(\"True 1, identified as 0: \")\n",
    "    preds_0 = np.where(predictions == 0)[0]\n",
    "    counter = 0\n",
    "    for i in range(len(preds_0)):\n",
    "        if preds_0[i] in true_1:\n",
    "            counter += 1\n",
    "    print(counter)\n",
    "    \n",
    "    \n",
    "    print(\"Number of true 2: \")\n",
    "    true_2 = np.where(true_values == 2)[0]\n",
    "    print(len(true_2))\n",
    "    print(\"Correctly identified as 2: \")\n",
    "    preds_2 = np.where(predictions == 2)[0]\n",
    "    \n",
    "    counter = 0\n",
    "    for i in range(len(preds_2)):\n",
    "        if preds_2[i] in true_2:\n",
    "            counter += 1\n",
    "    print(counter)\n",
    "    print(\"Predicted # 2: \")\n",
    "    print(len(preds_2))\n",
    "    \n",
    "    print(\"True 2, identified as 0 or 1: \")\n",
    "    preds_2 = np.where(predictions != 2)[0]\n",
    "    counter = 0\n",
    "    for i in range(len(preds_2)):\n",
    "        if preds_2[i] in true_2:\n",
    "            counter += 1\n",
    "    print(counter)\n",
    "    \n",
    "    print(\"True 1, identified as 2: \")\n",
    "    preds_2 = np.where(predictions == 2)[0]\n",
    "    counter = 0\n",
    "    for i in range(len(preds_0)):\n",
    "        if preds_2[i] in true_1:\n",
    "            counter += 1\n",
    "    print(counter)\n",
    "    \n",
    "    print(\"True 0, identified as 2: \")\n",
    "    preds_2 = np.where(predictions == 2)[0]\n",
    "    counter = 0\n",
    "    for i in range(len(preds_2)):\n",
    "        if preds_2[i] in true_0:\n",
    "            counter += 1\n",
    "    print(counter)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### features = # some features\n",
    "### forward pass of the whole network is ..\n",
    "## logit = model(graph, features)\n",
    "output_of_first_gatconv, attention_weights = model.gat_layers[0](my_dgl_graph, features, get_attention=True)\n",
    "output_of_first_gatconv = output_of_first_gatconv.flatten(1)\n",
    "output_of_second_gatconv, attention_weights2 = model.gat_layers[1](my_dgl_graph, output_of_first_gatconv, get_attention=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where I got the important edges! It says important nodes, but it actually is the edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_important_nodes(attention_weights, top_k=50):\n",
    "    # Check if attention_weights has the shape [num_nodes, num_heads, 1]\n",
    "    assert attention_weights.shape[2] == 1, \"Expected attention_weights shape to be [num_nodes, num_heads, 1]\"\n",
    "\n",
    "    # Squeeze the last dimension to get shape [num_nodes, num_heads]\n",
    "    attention_weights = attention_weights.squeeze(dim=2)  # Shape: [num_nodes, num_heads]\n",
    "\n",
    "    # Sum the attention weights across all heads\n",
    "    attention_sums = attention_weights.sum(dim=1)  # Shape: [num_nodes]\n",
    "\n",
    "    # Get the top_k nodes with the highest attention sums\n",
    "    if attention_sums.numel() > top_k:  # Ensure there are enough nodes\n",
    "        important_nodes = torch.topk(attention_sums, top_k).indices\n",
    "    else:\n",
    "        important_nodes = torch.arange(attention_sums.numel())  # All nodes if less than top_k\n",
    "    \n",
    "    return important_nodes.cpu().numpy()\n",
    "\n",
    "important_nodes = get_important_nodes(attention_weights)\n",
    "important_nodes2 = get_important_nodes(attention_weights2)\n",
    "print(f'Most important nodes: {important_nodes}')\n",
    "print(f'Most important nodes: {important_nodes2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting predictions and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions_and_labels(g, features, mask, model, true_labels):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(g, features)\n",
    "        logits = logits[mask]\n",
    "        _, predicted = torch.max(logits, dim=1)\n",
    "        predictions = predicted.cpu().numpy()\n",
    "        if isinstance(true_labels, torch.Tensor):\n",
    "            true_labels = true_labels[mask].cpu().numpy()\n",
    "        else:\n",
    "            true_labels = true_labels[mask]  # Assuming true_labels is already a NumPy array\n",
    "        return predictions, true_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code for GAT figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.metrics import f1_score, roc_auc_score, roc_curve, auc, confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "# Assuming the number of classes\n",
    "n_classes = len(set(true_labels))\n",
    "\n",
    "# Binarize the true labels\n",
    "true_labels_binarized = label_binarize(true_labels, classes=range(n_classes))\n",
    "\n",
    "# Calculate F1 score\n",
    "f1 = f1_score(true_labels, predictions, average='weighted')\n",
    "\n",
    "# Calculate AUC for multi-class classification\n",
    "with torch.no_grad():\n",
    "    logits = model(my_dgl_graph, features)\n",
    "    probs = torch.nn.functional.softmax(logits[masks[2]], dim=1)\n",
    "    positive_probs = probs.cpu().numpy()  # Multi-class probabilities\n",
    "\n",
    "# Calculate ROC curve and AUC for each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(true_labels_binarized[:, i], positive_probs[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Plotting F1 score and AUC\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.scatter(true_labels, predictions, alpha=0.5, color=\"#2E6A57\")\n",
    "plt.xlabel(\"True Labels\")\n",
    "plt.ylabel(\"Predicted Labels\")\n",
    "plt.title(f\"F1 Score: {f1:.2f} | AUC: {sum(roc_auc.values())/n_classes:.2f}\")\n",
    "plt.grid(False)\n",
    "plt.savefig('F1_AUC_Plot.png')\n",
    "plt.show()\n",
    "\n",
    "colors = ['#142C5F', '#7E8737', '#F6A895', '#124E63', '#FFBAD7']\n",
    "\n",
    "# Plot ROC Curve for each class\n",
    "plt.figure(figsize=(6, 4))\n",
    "for i in range(n_classes):\n",
    "    plt.plot(fpr[i], tpr[i], color=colors[i % len(colors)], lw=2, label=f'Class {i} (AUC = {roc_auc[i]:.2f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(False)\n",
    "plt.savefig('ROC_Curve.pdf')\n",
    "plt.show()\n",
    "\n",
    "# Calculate the Confusion Matrix\n",
    "conf_matrix = confusion_matrix(true_labels, predictions)\n",
    "\n",
    "# Display the Confusion Matrix\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=range(n_classes))\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.savefig('Confusion_Matrix.pdf')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
